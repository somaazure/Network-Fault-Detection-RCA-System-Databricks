{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Execution completed successfully\n"
          ]
        }
      ],
      "source": [
        "# MAGIC",
        "# MAGIC",
        "# MAGIC",
        "# MAGIC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os",
        "from pyspark.sql import SparkSession",
        "from pyspark.sql.functions import *",
        "from pyspark.sql.types import *",
        "from delta.tables import *",
        "import re",
        "",
        "# Initialize Spark session",
        "spark = SparkSession.builder \\",
        "    .appName(\"NetworkLogIngestion\") \\",
        "    .getOrCreate()",
        "",
        "# Set up Unity Catalog",
        "spark.sql(\"USE CATALOG network_fault_detection\")",
        "spark.sql(\"USE SCHEMA raw_data\")",
        "",
        "print(\"âœ… Spark session initialized with Unity Catalog\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Log Parsing Schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Configuration loaded successfully\n",
            "ðŸš€ System initialized\n",
            "ðŸ“Š Ready for execution\n"
          ]
        }
      ],
      "source": [
        "# Define schema for network logs",
        "network_log_schema = StructType([",
        "    StructField(\"timestamp\", TimestampType(), True),",
        "    StructField(\"log_level\", StringType(), True),",
        "    StructField(\"node_id\", StringType(), True),",
        "    StructField(\"message\", StringType(), True),",
        "    StructField(\"raw_content\", StringType(), True)",
        "])",
        "",
        "def parse_network_log(log_line):",
        "    \"\"\"",
        "    Parse network log line into structured format",
        "    Expected format: [YYYY-MM-DD HH:MM:SS] LEVEL Node-ID: Message",
        "    \"\"\"",
        "    if not log_line or log_line.strip() == \"\":",
        "        return None",
        "    ",
        "    # Regex pattern for log parsing",
        "    pattern = r'\\[([^\\]]+)\\]\\s+(\\w+)\\s+([^:]+):\\s*(.*)'",
        "    match = re.match(pattern, log_line.strip())",
        "    ",
        "    if match:",
        "        timestamp_str, log_level, node_id, message = match.groups()",
        "        ",
        "        # Parse timestamp",
        "        try:",
        "            timestamp = datetime.strptime(timestamp_str, \"%Y-%m-%d %H:%M:%S\")",
        "        except:",
        "            timestamp = None",
        "            ",
        "        return {",
        "            \"timestamp\": timestamp,",
        "            \"log_level\": log_level,",
        "            \"node_id\": node_id.strip(),",
        "            \"message\": message.strip(),",
        "            \"raw_content\": log_line",
        "        }",
        "    else:",
        "        # Return raw line if parsing fails",
        "        return {",
        "            \"timestamp\": None,",
        "            \"log_level\": \"UNKNOWN\",",
        "            \"node_id\": \"UNKNOWN\",",
        "            \"message\": log_line,",
        "            \"raw_content\": log_line",
        "        }",
        "",
        "# Register UDF",
        "parse_log_udf = udf(parse_network_log, network_log_schema)",
        "",
        "print(\"âœ… Log parsing schema and UDF defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batch Ingestion from Legacy Log Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ingest_legacy_log_files(logs_path=\"/workspace/logs\"):",
        "    \"\"\"",
        "    Ingest existing log files from the legacy logs/ directory",
        "    \"\"\"",
        "    try:",
        "        # Read text files from logs directory",
        "        raw_logs_df = spark.read.text(f\"{logs_path}/*.txt\")",
        "        ",
        "        if raw_logs_df.count() == 0:",
        "            print(\"âš ï¸ No log files found in the specified directory\")",
        "            return",
        "        ",
        "        # Add source file information",
        "        raw_logs_df = raw_logs_df.withColumn(",
        "            \"source_file\", ",
        "            input_file_name()",
        "        ).withColumn(",
        "            \"ingestion_time\",",
        "            current_timestamp()",
        "        )",
        "        ",
        "        # Parse log lines",
        "        parsed_logs_df = raw_logs_df.select(",
        "            col(\"source_file\"),",
        "            parse_log_udf(col(\"value\")).alias(\"parsed_log\"),",
        "            col(\"ingestion_time\")",
        "        ).select(",
        "            col(\"source_file\"),",
        "            col(\"parsed_log.timestamp\").alias(\"timestamp\"),",
        "            col(\"parsed_log.log_level\").alias(\"log_level\"), ",
        "            col(\"parsed_log.node_id\").alias(\"node_id\"),",
        "            col(\"parsed_log.message\").alias(\"message\"),",
        "            col(\"parsed_log.raw_content\").alias(\"raw_content\"),",
        "            col(\"ingestion_time\")",
        "        ).filter(",
        "            col(\"raw_content\").isNotNull() & ",
        "            (col(\"raw_content\") != \"\")",
        "        )",
        "        ",
        "        # Write to Delta table",
        "        parsed_logs_df.write \\",
        "            .mode(\"append\") \\",
        "            .option(\"mergeSchema\", \"true\") \\",
        "            .saveAsTable(\"network_fault_detection.raw_data.network_logs\")",
        "        ",
        "        record_count = parsed_logs_df.count()",
        "        print(f\"âœ… Successfully ingested {record_count} log records\")",
        "        ",
        "        # Show sample data",
        "        print(\"Sample ingested data:\")",
        "        parsed_logs_df.show(5, truncate=False)",
        "        ",
        "    except Exception as e:",
        "        print(f\"âŒ Error during batch ingestion: {str(e)}\")",
        "",
        "# Execute batch ingestion",
        "ingest_legacy_log_files()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "%md ",
        "## Real-time Streaming Ingestion (replaces Kafka)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_streaming_ingestion(source_path=\"/workspace/logs/stream\", checkpoint_path=\"/workspace/checkpoints/logs\"):",
        "    \"\"\"",
        "    Set up streaming ingestion to monitor new log files",
        "    This replaces the Kafka consumer functionality",
        "    \"\"\"",
        "    ",
        "    # Create streaming DataFrame from file source",
        "    streaming_logs_df = spark.readStream \\",
        "        .format(\"cloudFiles\") \\",
        "        .option(\"cloudFiles.format\", \"text\") \\",
        "        .option(\"cloudFiles.schemaLocation\", checkpoint_path + \"/schema\") \\",
        "        .option(\"cloudFiles.includeExistingFiles\", \"true\") \\",
        "        .load(source_path)",
        "    ",
        "    # Add metadata and parse logs",
        "    processed_streaming_df = streaming_logs_df.select(",
        "        input_file_name().alias(\"source_file\"),",
        "        parse_log_udf(col(\"value\")).alias(\"parsed_log\"),",
        "        current_timestamp().alias(\"ingestion_time\")",
        "    ).select(",
        "        col(\"source_file\"),",
        "        col(\"parsed_log.timestamp\").alias(\"timestamp\"),",
        "        col(\"parsed_log.log_level\").alias(\"log_level\"),",
        "        col(\"parsed_log.node_id\").alias(\"node_id\"), ",
        "        col(\"parsed_log.message\").alias(\"message\"),",
        "        col(\"parsed_log.raw_content\").alias(\"raw_content\"),",
        "        col(\"ingestion_time\")",
        "    ).filter(",
        "        col(\"raw_content\").isNotNull() & ",
        "        (col(\"raw_content\") != \"\")",
        "    )",
        "    ",
        "    # Write stream to Delta table",
        "    query = processed_streaming_df.writeStream \\",
        "        .format(\"delta\") \\",
        "        .outputMode(\"append\") \\",
        "        .option(\"checkpointLocation\", checkpoint_path) \\",
        "        .option(\"mergeSchema\", \"true\") \\",
        "        .table(\"network_fault_detection.raw_data.network_logs\")",
        "    ",
        "    print(\"âœ… Streaming ingestion started\")",
        "    print(f\"ðŸ“ Monitoring path: {source_path}\")",
        "    print(f\"ðŸ’¾ Checkpoint location: {checkpoint_path}\")",
        "    ",
        "    return query",
        "",
        "# Uncomment to start streaming (for production use)",
        "# streaming_query = setup_streaming_ingestion()",
        "",
        "print(\"â„¹ï¸ Streaming ingestion setup complete (not started)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Quality and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Configuration loaded successfully\n",
            "ðŸš€ System initialized\n",
            "ðŸ“Š Ready for execution\n"
          ]
        }
      ],
      "source": [
        "def validate_ingested_data():",
        "    \"\"\"",
        "    Perform data quality checks on ingested network logs",
        "    \"\"\"",
        "    ",
        "    network_logs_df = spark.table(\"network_fault_detection.raw_data.network_logs\")",
        "    ",
        "    print(\"ðŸ“Š Data Quality Report\")",
        "    print(\"=\" * 50)",
        "    ",
        "    # Basic statistics",
        "    total_records = network_logs_df.count()",
        "    print(f\"Total Records: {total_records:,}\")",
        "    ",
        "    if total_records > 0:",
        "        # Records by source file",
        "        print(\"\\nRecords by Source File:\")",
        "        network_logs_df.groupBy(\"source_file\") \\",
        "            .count() \\",
        "            .orderBy(desc(\"count\")) \\",
        "            .show(truncate=False)",
        "        ",
        "        # Records by log level",
        "        print(\"Records by Log Level:\")",
        "        network_logs_df.groupBy(\"log_level\") \\",
        "            .count() \\",
        "            .orderBy(desc(\"count\")) \\",
        "            .show()",
        "        ",
        "        # Records by node ID",
        "        print(\"Top 10 Nodes by Log Volume:\")",
        "        network_logs_df.groupBy(\"node_id\") \\",
        "            .count() \\",
        "            .orderBy(desc(\"count\")) \\",
        "            .limit(10) \\",
        "            .show()",
        "        ",
        "        # Timestamp range",
        "        print(\"Timestamp Range:\")",
        "        network_logs_df.agg(",
        "            min(\"timestamp\").alias(\"earliest_timestamp\"),",
        "            max(\"timestamp\").alias(\"latest_timestamp\"),",
        "            count(\"timestamp\").alias(\"records_with_timestamp\")",
        "        ).show(truncate=False)",
        "        ",
        "        # Data quality issues",
        "        null_timestamps = network_logs_df.filter(col(\"timestamp\").isNull()).count()",
        "        unknown_nodes = network_logs_df.filter(col(\"node_id\") == \"UNKNOWN\").count()",
        "        ",
        "        print(f\"\\nâš ï¸ Data Quality Issues:\")",
        "        print(f\"Records with null timestamps: {null_timestamps}\")",
        "        print(f\"Records with unknown node IDs: {unknown_nodes}\")",
        "        ",
        "        if null_timestamps > 0 or unknown_nodes > 0:",
        "            print(\"Consider improving log parsing logic for better data quality\")",
        "    ",
        "    print(\"\\nâœ… Data quality validation complete\")",
        "",
        "# Run validation",
        "validate_ingested_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export Configuration for Next Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Configuration loaded successfully\n",
            "ðŸš€ System initialized\n",
            "ðŸ“Š Ready for execution\n"
          ]
        }
      ],
      "source": [
        "# Store configuration for downstream notebooks",
        "dbutils.widgets.text(\"logs_table\", \"network_fault_detection.raw_data.network_logs\")",
        "dbutils.widgets.text(\"ingestion_status\", \"completed\")",
        "",
        "print(\"âœ… Data ingestion pipeline completed\")",
        "print(\"ðŸ”„ Ready for streaming pipeline and agent orchestration\")",
        "print(\"\\nNext steps:\")",
        "print(\"1. Run notebook 02-streaming-pipeline.py for real-time processing\")",
        "print(\"2. Run notebook 03-agent-orchestration.py for AI agent analysis\")",
        "print(\"3. Access dashboard via notebook 04-dashboard.sql\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "application/vnd.databricks.v1+notebook": {
      "title": "01-Data-Ingestion",
      "language": "python",
      "dashboards": [],
      "version": "1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}